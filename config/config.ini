[optimizer]
# The optimization technique to use. Options: ga, pso, hybrid
technique = ga

[ga]
# The number of individuals (enrichment patterns) in the population. A larger population explores more but is slower. (Default: 1500)
population_size = 1500
# The number of generations the GA will run using ML predictors before triggering a full OpenMC verification. (Default: 1000)
generations_per_openmc_cycle = 1000
# Choose the crossover method. Options: single_point, zone_based, blend
crossover_method = zone_based
# Base probability (0.0 to 1.0) that a gene (enrichment value) will be mutated. (Default: 0.20)
mutation_rate = 0.20
# The maximum mutation rate that the adaptive algorithm can reach. Should not be too high. (Default: 0.30)
max_mutation_rate = 0.30
# Probability (0.0 to 1.0) that two parents will perform crossover. (Default: 0.85)
crossover_rate = 0.85
# Maximum crossover rate for the adaptive algorithm. (Default: 0.90)
max_crossover_rate = 0.90
# Number of top individuals guaranteed to survive to the next generation. (Default: 10)
elitism_count = 50
# Number of generations with no improvement before mutation/crossover rates are adaptively increased. (Default: 50)
stagnation_threshold = 50
# If population diversity (average difference between individuals) falls below this, rates are increased. (Default: 0.3)
diversity_threshold = 0.3
# Number of individuals that compete in a selection tournament. The winner becomes a parent. (Default: 20)
tournament_size = 20

[pso]
# Number of particles in the swarm. Analogous to GA's population_size. (Default: 1500)
swarm_size = 1500
# Number of iterations the PSO will run using ML predictors before an OpenMC verification. (Default: 1000)
iterations_per_openmc_cycle = 1000
# Cognitive coefficient (c1), controls the particle's attraction to its personal best. (Default: 2.0)
cognitive_coeff = 2.0
# Social coefficient (c2), controls the particle's attraction to the global/neighborhood best. (Default: 2.0)
social_coeff = 2.0
# Number of iterations without improvement before the PSO cycle exits early.
pso_convergence_threshold = 1200
# Starting and ending inertia weights for linear decay.
inertia_weight_start = 0.95
inertia_weight_end = 0.35

# ADVANCED PSO PARAMETERS
# Topology defines how particles are connected. Options: global, ring, random, fitness_based
topology = ring
# For 'ring', 'random', or 'fitness_based' topologies, the number of neighbors for each particle. (e.g., 2, 4)
neighborhood_size = 4
# Frequency (in iterations) to rebuild neighborhoods for 'random' and 'fitness_based' topologies.
neighborhood_rebuild_frequency = 100

# Set to true to enable adaptive velocity clamping, false to use a fixed max_change_probability.
adaptive_velocity = true
# Base and max probability of change, used for adaptive velocity clamping.
base_change_probability = 0.25
max_change_probability = 0.90

# Set to true to enable dynamic exploration factor, false to use a fixed pso_exploration_factor.
dynamic_exploration = true
# Base factor for random exploration. Used as a baseline for the dynamic calculation.
base_exploration_factor = 0.12
# If swarm diversity falls below this, exploration is boosted. (Used by dynamic_exploration)
diversity_threshold = 0.20

# Multi-swarm parameters
enable_multi_swarm = true
num_sub_swarms = 3
migration_frequency = 200
migration_rate = 0.05

# Moderated Local Search
enable_local_search = true
# How often (in iterations) to attempt local search.
local_search_frequency = 50

# Set to true to enable GA-style smart mutation in PSO.
enable_smart_mutation = true
# The probability (0.5 to 1.0) of 'smart_mutate' choosing a beneficial mutation direction.
smart_mutation_bias = 0.75

# Fixed probability of change. Only used if adaptive_velocity = false.
# max_velocity_fraction = 0.2
# Fixed exploration factor. Only used if dynamic_exploration = false.
# pso_exploration_factor = 0.1

[hybrid]
# Defines the strategy for switching between GA and PSO.
# Options:
#   fixed_cycles: Switch GA -> PSO after a set number of GA cycles.
#   stagnation: Switch GA -> PSO when GA fitness stagnates AND diversity drops.
#   oscillate: Switch between GA and PSO based on stagnation in each phase.
#   adaptive: Switch automatically based on which phase is performing better.
switch_mode = oscillate

# Parameters for 'fixed_cycles' and 'oscillate' modes
ga_phase_cycles = 5
pso_phase_cycles = 5

# Parameters for 'stagnation' mode
# Number of cycles with no fitness improvement to be considered stagnation.
stagnation_threshold = 10
# Diversity threshold below which the GA is considered to have converged.
ga_min_diversity_for_switch = 0.25
# Diversity threshold below which PSO is considered to have converged.
pso_min_diversity_for_switch = 0.15

# Parameters for Seeding
# The fraction of the GA population to be seeded with the best individuals from PSO.
ga_seed_ratio = 0.25
# The fraction of the PSO swarm to be seeded with the best individuals from GA.
pso_seed_ratio = 0.75

# Parameters for 'adaptive' mode 
# Switch to the other algorithm if its average fitness gain is this much better (e.g., 1.2 = 20% better).
adaptive_switching_threshold = 1.2
# The minimum number of cycles a phase must run before an adaptive switch can occur.
min_adaptive_phase_duration = 5
# Factor for comparing negative trends in adaptive mode. See hybrid_engine.py for usage.
adaptive_trend_dampening_factor = 0.5

[fitness_tuning]
# The weight (0.0 to 1.0) for the k-effective component of the fitness score.
keff_fitness_weight = 0.4
# The weight (0.0 to 1.0) for the PPF component of the fitness score. Must sum to 1.0 with keff_fitness_weight.
ppf_fitness_weight = 0.6
# Penalty factor in the fitness function when k-effective is outside the target tolerance. (Default: 20)
keff_penalty_factor = 20
# K-effective difference threshold above which fitness is heavily weighted towards improving k-effective. (Default: 0.01)
high_keff_diff_threshold = 0.01
# K-effective difference threshold for a balanced fitness weight. (Default: 0.005)
med_keff_diff_threshold = 0.005

[ga_tuning]
# Frequency (in generations) to log GA progress. (Default: 50)
log_frequency = 50
# Number of individuals to sample when calculating diversity. (Default: 50)
diversity_sample_size = 50
# Factor to increase mutation rate when k-effective is far from the target. (Default: 1.5)
smart_mutate_increase_factor = 1.5
# Factor to decrease mutation rate when k-effective is close to the target. (Default: 0.8)
smart_mutate_decrease_factor = 0.8
# Number of generations without improvement in predicted fitness before the GA cycle exits early.
convergence_threshold = 1000
# The probability (0.5 to 1.0) of 'smart_mutate' choosing a beneficial mutation direction when keff is near the target.
smart_mutation_bias = 0.75
# Sets the diversity check to run every 'log_frequency * diversity_check_multiplier' generations. keep it to 1.
diversity_check_multiplier = 1
# The alpha parameter for Blend Crossover (BLX-Î±). Controls the exploration range. (Default: 0.5)
blend_crossover_alpha = 0.5

[enrichment]
# Defines the available enrichment values for central assemblies. Format: start, stop, step.
# Example: If your reactor's nominal enrichment is 16%, try a range below and around it.
central_range = 14.0, 15.5, 0.1
# Defines the available enrichment values for outer assemblies. Format: start, stop, step.
# Example: Try a range above and around the nominal enrichment.
outer_range = 14.5, 18.0, 0.1
# Number of initial OpenMC simulations to run if 'initial_configs' is empty. At least 100 is recommended for a good baseline.
initial_samples = 100
# **HIGHLY RECOMMENDED**: A specific list of (central, outer) enrichment pairs for initial data generation.
# This overrides 'initial_samples' and gives you full control. Ensure these points cover your search space well.
#initial_configs = 
[simulation]
# **CRITICAL**: The target k-effective value for your reactor design.
target_keff = 1.12371651800807
# The total number of full OpenMC verification cycles to run. (Default: 300)
num_cycles = 500
# **CRITICAL**: The total number of fuel assemblies in your reactor model.
num_assemblies = 150
# **CRITICAL**: The number of fuel assemblies in the central region (determined in Step 3).
num_central_assemblies = 54
# **CRITICAL**: The starting material ID for the first fuel assembly in materials.xml (determined in Step 1).
start_id = 3
# **CRITICAL**: Path to the OpenMC materials file.
materials_xml_path = materials.xml
# **CRITICAL**: Name of the fission tally in tallies.xml (determined in Step 2).
fission_tally_name = fission_in_fuel_cells
# File path for the "live" (unvalidated) PPF dataset.
ppf_interp_file_live = data/ppf_interp_data.json
# File path for the "best" (validated) PPF dataset.
ppf_interp_file_best = data/ppf_interp_data_best.json
# File path for the "live" (unvalidated) Keff dataset.
keff_interp_file_live = data/keff_interp_data.json
# File path for the "best" (validated) Keff dataset.
keff_interp_file_best = data/keff_interp_data_best.json
# File path for the main checkpoint file that saves the GA's overall state.
checkpoint_file = data/ga_checkpoint.json
# The glob pattern to find the latest OpenMC statepoint file.
statepoint_filename_pattern = statepoint.*.h5
# Number of times to retry a failed OpenMC simulation.
openmc_retries = 2
# Delay in seconds between retries.
openmc_retry_delay = 5

[hardware]
# Set to 1 to enable CPU-based ML models (scikit-learn), 0 to disable.
cpu = 1
# Set to 1 to enable GPU-based ML models (cuML). If both are 1, GPU is preferred. Currently, it's better to not use gpu.
gpu = 0

[interpolator]
# Maximum number of data points to keep for the k-effective interpolator.
max_keff_points = 100000
# Maximum number of data points to keep for the PPF interpolator.
max_ppf_points = 100000
# Minimum number of data points required before the interpolators start making predictions.
min_interp_points = 20
# Minimum R^2 validation score required to accept a retrained model. Prevents model degradation.
min_validation_score = 0.5
# Type of regressor for PPF prediction. Options: knn, random_forest, ridge, dnn, gbm
ppf_regressor_type = dnn
# Type of regressor for Keff prediction. Options: knn, random_forest, ridge, dnn, gbm
keff_regressor_type = gbm

# Hyperparameters for scikit-learn / cuML regressors

# For 'knn'
n_neighbors = 5

# For 'random_forest'
rf_n_estimators = 100
rf_max_depth = 0

# For 'gbm' (Gradient Boosting Machine)
gbm_n_estimators = 400
gbm_learning_rate = 0.05

# ==================== KEFF DNN HYPERPARAMETERS ====================
# Hidden layers for Keff DNN. Example: 64, 64 creates two hidden layers.
# Increasing: More layers/neurons can capture more complex patterns (higher model capacity). Risks overfitting and longer training time.
# Decreasing: Simpler model, faster training, less prone to overfitting. May underfit if the problem is complex.
keff_nn_hidden_layers = 128, 160

# Number of training epochs for Keff DNN.
# Increasing: Allows the model more passes over the data to learn. Risks overfitting if too high.
# Decreasing: Faster training, but may result in an undertrained (underfit) model.
keff_nn_epochs = 250

# Batch size for Keff DNN training.
# Increasing: Faster training epochs and more stable gradient updates. Requires more memory. Can sometimes lead to poorer generalization.
# Decreasing: Slower training and noisier gradients, which can sometimes help escape local minima. Requires less memory.
keff_nn_batch_size = 32

# Learning rate for Keff DNN optimizer.
# Increasing: Faster convergence, but risks overshooting the optimal solution and becoming unstable.
# Decreasing: Slower, more reliable convergence. May get stuck in local minima if too small.
keff_nn_learning_rate = 0.005

# Dropout rate for Keff DNN (0.0 to 1.0). A regularization technique.
# Increasing: Stronger regularization effect, forces the network to learn more robust features. Helps prevent overfitting. If too high, can lead to underfitting.
# Decreasing: Weaker regularization. Set to 0 to disable.
keff_nn_dropout_rate = 0.3

# Early stopping patience for Keff DNN.
# Increasing: The model will wait for more epochs without improvement before stopping. Useful if training progress is noisy.
# Decreasing: Stops training more quickly once performance on the validation set plateaus.
keff_nn_patience = 15

# Random seed for Keff DNN reproducibility.
keff_nn_random_seed = 42

# ==================== PPF DNN HYPERPARAMETERS ====================
# Hidden layers for PPF DNN. Example: 160 creates one hidden layer.
# Increasing: More model capacity to learn complex relationships between enrichment and power peaking. Risks overfitting.
# Decreasing: Simpler, faster model. May not be powerful enough to capture the underlying physics accurately.
ppf_nn_hidden_layers = 128, 32

# Number of training epochs for PPF DNN.
# Increasing: More training time, but risks overfitting.
# Decreasing: Less training time, but risks underfitting.
ppf_nn_epochs = 200

# Batch size for PPF DNN training.
# Increasing: Speeds up training, uses more memory.
# Decreasing: Slows down training, uses less memory, can add beneficial noise.
ppf_nn_batch_size = 32

# Learning rate for PPF DNN optimizer.
# Increasing: Converges faster, but may be unstable.
# Decreasing: More stable, but converges slower.
ppf_nn_learning_rate = 0.005

# Dropout rate for PPF DNN (0.0 to 1.0).
# Increasing: Stronger prevention against overfitting.
# Decreasing: Weaker prevention against overfitting.
ppf_nn_dropout_rate = 0.2

# Early stopping patience for PPF DNN.
# Increasing: More tolerant to temporary plateaus in performance.
# Decreasing: Stops sooner when performance isn't improving.
ppf_nn_patience = 15

# Random seed for PPF DNN reproducibility.
ppf_nn_random_seed = 42
